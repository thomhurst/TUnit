"use strict";(self.webpackChunktunit_docs_site=self.webpackChunktunit_docs_site||[]).push([[1528],{1958:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"benchmarks/methodology","title":"Benchmark Methodology","description":"How TUnit\'s performance benchmarks are measured and compared","source":"@site/docs/benchmarks/methodology.md","sourceDirName":"benchmarks","slug":"/benchmarks/methodology","permalink":"/docs/benchmarks/methodology","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Benchmark Methodology","description":"How TUnit\'s performance benchmarks are measured and compared","sidebar_position":3},"sidebar":"docs","previous":{"title":"Performance Benchmarks","permalink":"/docs/benchmarks/"}}');var r=s(4848),t=s(8453);const l={title:"Benchmark Methodology",description:"How TUnit's performance benchmarks are measured and compared",sidebar_position:3},a="Benchmark Methodology",c={},d=[{value:"Core Principles",id:"core-principles",level:2},{value:"1. Real-World Scenarios",id:"1-real-world-scenarios",level:3},{value:"2. Fair Comparison",id:"2-fair-comparison",level:3},{value:"3. Statistical Rigor",id:"3-statistical-rigor",level:3},{value:"Test Categories",id:"test-categories",level:2},{value:"Runtime Benchmarks",id:"runtime-benchmarks",level:3},{value:"DataDrivenTests",id:"datadriventests",level:4},{value:"AsyncTests",id:"asynctests",level:4},{value:"ScaleTests",id:"scaletests",level:4},{value:"MatrixTests",id:"matrixtests",level:4},{value:"MassiveParallelTests",id:"massiveparalleltests",level:4},{value:"Build Benchmarks",id:"build-benchmarks",level:3},{value:"Environment",id:"environment",level:2},{value:"Hardware",id:"hardware",level:3},{value:"Software",id:"software",level:3},{value:"Configuration",id:"configuration",level:3},{value:"Measurement Process",id:"measurement-process",level:2},{value:"1. Build Phase",id:"1-build-phase",level:3},{value:"2. Execution Phase",id:"2-execution-phase",level:3},{value:"3. Analysis Phase",id:"3-analysis-phase",level:3},{value:"What Gets Measured",id:"what-gets-measured",level:2},{value:"Primary Metrics",id:"primary-metrics",level:3},{value:"Mean Execution Time",id:"mean-execution-time",level:4},{value:"Median Execution Time",id:"median-execution-time",level:4},{value:"Standard Deviation",id:"standard-deviation",level:4},{value:"Derived Metrics",id:"derived-metrics",level:3},{value:"Speedup Factor",id:"speedup-factor",level:4},{value:"AOT Improvement",id:"aot-improvement",level:4},{value:"Benchmark Automation",id:"benchmark-automation",level:2},{value:"Daily Execution",id:"daily-execution",level:3},{value:"Process",id:"process",level:3},{value:"Artifacts",id:"artifacts",level:3},{value:"Reproducibility",id:"reproducibility",level:2},{value:"Running Locally",id:"running-locally",level:3},{value:"Viewing Results",id:"viewing-results",level:3},{value:"Limitations &amp; Caveats",id:"limitations--caveats",level:2},{value:"What Benchmarks Don&#39;t Measure",id:"what-benchmarks-dont-measure",level:3},{value:"Variance Factors",id:"variance-factors",level:3},{value:"Interpreting Results",id:"interpreting-results",level:3},{value:"Transparency",id:"transparency",level:2},{value:"Open Source",id:"open-source",level:3},{value:"Community Verification",id:"community-verification",level:3},{value:"Further Reading",id:"further-reading",level:2}];function o(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"benchmark-methodology",children:"Benchmark Methodology"})}),"\n",(0,r.jsx)(n.p,{children:"This page explains how TUnit's performance benchmarks are conducted to ensure fair, accurate, and reproducible results."}),"\n",(0,r.jsx)(n.h2,{id:"core-principles",children:"Core Principles"}),"\n",(0,r.jsx)(n.h3,{id:"1-real-world-scenarios",children:"1. Real-World Scenarios"}),"\n",(0,r.jsx)(n.p,{children:"Benchmarks test realistic patterns, not artificial micro-benchmarks:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Actual assertion logic"}),"\n",(0,r.jsx)(n.li,{children:"Real data source patterns"}),"\n",(0,r.jsx)(n.li,{children:"Typical setup/teardown workflows"}),"\n",(0,r.jsx)(n.li,{children:"Common parallelization strategies"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"2-fair-comparison",children:"2. Fair Comparison"}),"\n",(0,r.jsx)(n.p,{children:"Every framework implements identical test logic:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Same test methods"}),"\n",(0,r.jsx)(n.li,{children:"Same data inputs"}),"\n",(0,r.jsx)(n.li,{children:"Same assertion complexity"}),"\n",(0,r.jsx)(n.li,{children:"Equivalent configuration"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"3-statistical-rigor",children:"3. Statistical Rigor"}),"\n",(0,r.jsxs)(n.p,{children:["All benchmarks use ",(0,r.jsx)(n.a,{href:"https://benchmarkdotnet.org/",children:"BenchmarkDotNet"}),", the industry-standard .NET benchmarking library:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Multiple iterations per benchmark"}),"\n",(0,r.jsx)(n.li,{children:"Statistical outlier detection"}),"\n",(0,r.jsx)(n.li,{children:"Warm-up phase excluded from measurements"}),"\n",(0,r.jsx)(n.li,{children:"Standard deviation and median reported"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"test-categories",children:"Test Categories"}),"\n",(0,r.jsx)(n.h3,{id:"runtime-benchmarks",children:"Runtime Benchmarks"}),"\n",(0,r.jsx)(n.h4,{id:"datadriventests",children:"DataDrivenTests"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),": Measure parameterized test performance"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"What's tested"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-csharp",children:"[Test]\n[Arguments(1, 2, 3)]\n[Arguments(4, 5, 9)]\n// ... 50 argument sets\npublic void TestAddition(int a, int b, int expected)\n{\n    Assert.That(a + b).IsEqualTo(expected);\n}\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Why it matters"}),": Most test suites use parameterized tests extensively."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h4,{id:"asynctests",children:"AsyncTests"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),": Measure async/await pattern performance"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"What's tested"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-csharp",children:"[Test]\npublic async Task TestAsyncOperation()\n{\n    var result = await SimulateAsyncWork();\n    await Assert.That(result).IsNotNull();\n}\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Why it matters"}),": Modern .NET is async-first."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h4,{id:"scaletests",children:"ScaleTests"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),": Measure scalability with large test counts"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"What's tested"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"1000+ test methods"}),"\n",(0,r.jsx)(n.li,{children:"Parallel execution"}),"\n",(0,r.jsx)(n.li,{children:"Memory efficiency"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Why it matters"}),": Enterprise codebases have thousands of tests."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h4,{id:"matrixtests",children:"MatrixTests"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),": Measure combinatorial test generation"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"What's tested"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-csharp",children:'[Test]\n[Matrix("Create", "Update", "Delete")] // Operation\n[Matrix("User", "Admin", "Guest")]     // Role\npublic void TestPermissions(string op, string role)\n{\n    // 9 test combinations\n}\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Why it matters"}),": Matrix testing is common for comprehensive coverage."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h4,{id:"massiveparalleltests",children:"MassiveParallelTests"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),": Stress test parallel execution"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"What's tested"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"100+ tests running concurrently"}),"\n",(0,r.jsx)(n.li,{children:"Resource contention"}),"\n",(0,r.jsx)(n.li,{children:"Thread safety"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Why it matters"}),": Parallel execution is TUnit's default behavior."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h3,{id:"build-benchmarks",children:"Build Benchmarks"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Purpose"}),": Measure compilation time impact"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"What's tested"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Clean build time"}),"\n",(0,r.jsx)(n.li,{children:"Incremental build time"}),"\n",(0,r.jsx)(n.li,{children:"Source generator overhead"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Why it matters"}),": Fast builds improve developer productivity."]}),"\n",(0,r.jsx)(n.h2,{id:"environment",children:"Environment"}),"\n",(0,r.jsx)(n.h3,{id:"hardware",children:"Hardware"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Platform"}),": GitHub Actions Ubuntu runners"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Consistency"}),": Same hardware for all frameworks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reproducibility"}),": Daily automated runs"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"software",children:"Software"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Framework Versions"}),": Latest stable releases"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:".NET Version"}),": .NET 10 (latest)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"OS"}),": Ubuntu Latest"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"configuration",children:"Configuration"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Release Mode"}),": All tests compiled with optimizations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Native AOT"}),": Separate TUnit_AOT benchmark"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Default Settings"}),": No special framework configuration"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"measurement-process",children:"Measurement Process"}),"\n",(0,r.jsx)(n.h3,{id:"1-build-phase",children:"1. Build Phase"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Build all frameworks identically\ndotnet build -c Release -p:TestFramework=TUNIT\ndotnet build -c Release -p:TestFramework=XUNIT3\ndotnet build -c Release -p:TestFramework=NUNIT\ndotnet build -c Release -p:TestFramework=MSTEST\n"})}),"\n",(0,r.jsx)(n.h3,{id:"2-execution-phase",children:"2. Execution Phase"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-csharp",children:'[Benchmark]\npublic async Task TUnit()\n{\n    await Cli.Wrap("UnifiedTests.exe")\n        .WithArguments(["--filter", "TestCategory"])\n        .ExecuteBufferedAsync();\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"3-analysis-phase",children:"3. Analysis Phase"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"BenchmarkDotNet collects metrics"}),"\n",(0,r.jsx)(n.li,{children:"Statistical analysis performed"}),"\n",(0,r.jsx)(n.li,{children:"Results exported to markdown"}),"\n",(0,r.jsx)(n.li,{children:"Historical trends tracked"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"what-gets-measured",children:"What Gets Measured"}),"\n",(0,r.jsx)(n.h3,{id:"primary-metrics",children:"Primary Metrics"}),"\n",(0,r.jsx)(n.h4,{id:"mean-execution-time",children:"Mean Execution Time"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Definition"}),": Average time across all iterations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unit"}),": Milliseconds (ms) or Seconds (s)"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"Lower is better"})}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"median-execution-time",children:"Median Execution Time"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Definition"}),": Middle value, less affected by outliers"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unit"}),": Milliseconds (ms) or Seconds (s)"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"More stable than mean"})}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"standard-deviation",children:"Standard Deviation"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Definition"}),": Measure of result consistency"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Unit"}),": Same as mean"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lower is better"})," (more consistent)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"derived-metrics",children:"Derived Metrics"}),"\n",(0,r.jsx)(n.h4,{id:"speedup-factor",children:"Speedup Factor"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Speedup = (Other Framework Time) / (TUnit Time)\n"})}),"\n",(0,r.jsx)(n.p,{children:'Example: "2.5x faster" means TUnit is 2.5 times faster.'}),"\n",(0,r.jsx)(n.h4,{id:"aot-improvement",children:"AOT Improvement"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"AOT Speedup = (TUnit JIT Time) / (TUnit AOT Time)\n"})}),"\n",(0,r.jsx)(n.p,{children:'Example: "4x faster with AOT" means Native AOT is 4 times faster than JIT.'}),"\n",(0,r.jsx)(n.h2,{id:"benchmark-automation",children:"Benchmark Automation"}),"\n",(0,r.jsx)(n.h3,{id:"daily-execution",children:"Daily Execution"}),"\n",(0,r.jsxs)(n.p,{children:["Benchmarks run automatically every 24 hours via ",(0,r.jsx)(n.a,{href:"https://github.com/thomhurst/TUnit/blob/main/.github/workflows/speed-comparison.yml",children:"GitHub Actions"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"process",children:"Process"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Build"}),": Compile all framework versions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Execute"}),": Run benchmarks in isolated processes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Analyze"}),": Parse BenchmarkDotNet output"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Publish"}),": Update documentation automatically"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Track"}),": Store historical trends"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"artifacts",children:"Artifacts"}),"\n",(0,r.jsx)(n.p,{children:"All raw benchmark results are available as GitHub Actions artifacts for 90 days."}),"\n",(0,r.jsx)(n.h2,{id:"reproducibility",children:"Reproducibility"}),"\n",(0,r.jsx)(n.h3,{id:"running-locally",children:"Running Locally"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# 1. Navigate to benchmark project\ncd tools/speed-comparison\n\n# 2. Build all frameworks\ndotnet build -c Release\n\n# 3. Run specific benchmark\ncd Tests.Benchmark\ndotnet run -c Release -- --filter "*RuntimeBenchmarks*"\n'})}),"\n",(0,r.jsx)(n.h3,{id:"viewing-results",children:"Viewing Results"}),"\n",(0,r.jsxs)(n.p,{children:["Results are generated in ",(0,r.jsx)(n.code,{children:"BenchmarkDotNet.Artifacts/results/"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Markdown reports (*.md)"}),"\n",(0,r.jsx)(n.li,{children:"CSV data (*.csv)"}),"\n",(0,r.jsx)(n.li,{children:"HTML reports (*.html)"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"limitations--caveats",children:"Limitations & Caveats"}),"\n",(0,r.jsx)(n.h3,{id:"what-benchmarks-dont-measure",children:"What Benchmarks Don't Measure"}),"\n",(0,r.jsxs)(n.p,{children:["\u274c ",(0,r.jsx)(n.strong,{children:"IDE Integration"}),": Benchmarks don't measure test discovery in IDEs"]}),"\n",(0,r.jsxs)(n.p,{children:["\u274c ",(0,r.jsx)(n.strong,{children:"Debugger Performance"}),": Debug mode performance is not measured"]}),"\n",(0,r.jsxs)(n.p,{children:["\u274c ",(0,r.jsx)(n.strong,{children:"Real I/O"}),": Most tests use in-memory operations to avoid I/O variance"]}),"\n",(0,r.jsxs)(n.p,{children:["\u274c ",(0,r.jsx)(n.strong,{children:"External Dependencies"}),": No database, network, or file system calls"]}),"\n",(0,r.jsx)(n.h3,{id:"variance-factors",children:"Variance Factors"}),"\n",(0,r.jsx)(n.p,{children:"Results can vary based on:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Hardware configuration"}),"\n",(0,r.jsx)(n.li,{children:"Background processes"}),"\n",(0,r.jsx)(n.li,{children:"OS scheduling"}),"\n",(0,r.jsx)(n.li,{children:".NET runtime version"}),"\n",(0,r.jsx)(n.li,{children:"Test complexity"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"interpreting-results",children:"Interpreting Results"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Relative Performance"}),": Compare frameworks, not absolute times"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Your Mileage May Vary"}),": Real-world results depend on test characteristics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Trends Matter More"}),": Watch for performance regressions over time"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"transparency",children:"Transparency"}),"\n",(0,r.jsx)(n.h3,{id:"open-source",children:"Open Source"}),"\n",(0,r.jsx)(n.p,{children:"All benchmark code is open source:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/thomhurst/TUnit/tree/main/tools/speed-comparison/UnifiedTests",children:"Unified Test Suite"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/thomhurst/TUnit/tree/main/tools/speed-comparison/Tests.Benchmark",children:"Benchmark Harness"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/thomhurst/TUnit/blob/main/.github/workflows/speed-comparison.yml",children:"CI Workflow"})}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"community-verification",children:"Community Verification"}),"\n",(0,r.jsxs)(n.p,{children:["Found an issue with the benchmarks? ",(0,r.jsx)(n.a,{href:"https://github.com/thomhurst/TUnit/issues",children:"Open an issue"})," or submit a PR!"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://benchmarkdotnet.org/articles/overview.html",children:"BenchmarkDotNet Documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://learn.microsoft.com/en-us/dotnet/framework/performance/",children:".NET Performance Best Practices"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"/docs/advanced/performance-best-practices",children:"TUnit Performance Best Practices"})}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.em,{children:["Last updated: ",(new Date).toISOString().split("T")[0]]})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(o,{...e})}):o(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>a});var i=s(6540);const r={},t=i.createContext(r);function l(e){const n=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);