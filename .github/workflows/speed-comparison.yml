name: Speed Comparison

on:
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch:

jobs:
  build-test-artifacts:
    environment: ${{ github.ref == 'refs/heads/main' && 'Production' || 'Pull Requests' }}
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Setup .NET
        uses: actions/setup-dotnet@v5
        with:
          dotnet-version: 10.0.x

      - name: Build All Frameworks
        run: |
          dotnet build -c Release -p:TestFramework=TUNIT
          dotnet build -c Release -p:TestFramework=XUNIT3
          dotnet build -c Release -p:TestFramework=NUNIT
          dotnet build -c Release -p:TestFramework=MSTEST
        working-directory: "tools/speed-comparison/UnifiedTests"

      - name: Publish TUnit AOT
        run: |
          dotnet publish -c Release -p:TestFramework=TUNIT -p:Aot=true --use-current-runtime --output bin/Release-TUNIT-AOT/net10.0
        working-directory: "tools/speed-comparison/UnifiedTests"

      - name: Upload Build Artifacts
        uses: actions/upload-artifact@v5
        with:
          name: test-builds-ubuntu
          path: tools/speed-comparison/UnifiedTests/bin/
          retention-days: 1

  run-time-benchmarks:
    needs: build-test-artifacts
    environment: ${{ github.ref == 'refs/heads/main' && 'Production' || 'Pull Requests' }}
    strategy:
      matrix:
        class: [DataDrivenTests, AsyncTests, ScaleTests, MatrixTests, MassiveParallelTests, SetupTeardownTests]
      fail-fast: false
    runs-on: ubuntu-latest
    concurrency:
      group: "speed-comparison-run-time-${{matrix.class}}"
      cancel-in-progress: true

    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Setup .NET
        uses: actions/setup-dotnet@v5
        with:
          dotnet-version: 10.0.x

      - name: Download Build Artifacts
        uses: actions/download-artifact@v7
        with:
          name: test-builds-ubuntu
          path: tools/speed-comparison/UnifiedTests/bin/

      - name: Set Execute Permissions
        run: |
          find tools/speed-comparison/UnifiedTests/bin -type f -name "UnifiedTests" -exec chmod +x {} \; 2>/dev/null || true
          find tools/speed-comparison/UnifiedTests/bin -type f -name "xunit.v3.runner.console" -exec chmod +x {} \; 2>/dev/null || true
        working-directory: ${{ github.workspace }}

      - name: Run Benchmark
        run: dotnet run -c Release --framework net10.0 --allCategories=Runtime
        working-directory: "tools/speed-comparison/Tests.Benchmark"
        env:
          CLASS_NAME: ${{ matrix.class }}

      - name: Upload Markdown
        uses: actions/upload-artifact@v5
        if: always()
        with:
          name: ubuntu_markdown_run_time_${{ matrix.class }}
          path: |
            **/BenchmarkDotNet.Artifacts/**


  build-time-benchmarks:
    environment: ${{ github.ref == 'refs/heads/main' && 'Production' || 'Pull Requests' }}
    runs-on: ubuntu-latest
    concurrency:
      group: "speed-comparison-build-time"
      cancel-in-progress: true

    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Setup .NET
        uses: actions/setup-dotnet@v5
        with:
          dotnet-version: 10.0.x

      - name: Prepare builds
        run: |
          dotnet restore UnifiedTests/UnifiedTests.csproj
        working-directory: "tools/speed-comparison"

      - name: Run Benchmark
        run: dotnet run -c Release --allCategories=Build --framework net10.0
        working-directory: "tools/speed-comparison/Tests.Benchmark"

      - name: Upload Markdown
        uses: actions/upload-artifact@v5
        if: always()
        with:
          name: ubuntu_markdown_build_time
          path: |
            **/BenchmarkDotNet.Artifacts/**

  process-and-upload-benchmarks:
    needs: [run-time-benchmarks, build-time-benchmarks]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    permissions:
      contents: write
      pull-requests: write

    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 0
          token: ${{ secrets.ADMIN_TOKEN }}

      - name: Download All Runtime Benchmark Artifacts
        uses: actions/download-artifact@v7
        with:
          path: benchmark-results/runtime/
          pattern: ubuntu_markdown_run_time_*
          merge-multiple: false

      - name: Download Build Time Benchmark Artifacts
        uses: actions/download-artifact@v7
        with:
          path: benchmark-results/build/
          pattern: ubuntu_markdown_build_time
          merge-multiple: false

      - name: Setup Node.js
        uses: actions/setup-node@v6
        with:
          node-version: '24'

      - name: Process Benchmark Results
        run: |
          node .github/scripts/process-benchmarks.js

      - name: Upload Individual Runtime Benchmarks
        uses: actions/upload-artifact@v5
        if: always()
        with:
          name: benchmark-DataDrivenTests
          path: |
            docs/docs/benchmarks/DataDrivenTests.md
            docs/static/benchmarks/DataDrivenTests.json
          retention-days: 90

      - name: Upload Individual Runtime Benchmarks
        uses: actions/upload-artifact@v5
        if: always()
        with:
          name: benchmark-AsyncTests
          path: |
            docs/docs/benchmarks/AsyncTests.md
            docs/static/benchmarks/AsyncTests.json
          retention-days: 90

      - name: Upload Individual Runtime Benchmarks
        uses: actions/upload-artifact@v5
        if: always()
        with:
          name: benchmark-ScaleTests
          path: |
            docs/docs/benchmarks/ScaleTests.md
            docs/static/benchmarks/ScaleTests.json
          retention-days: 90

      - name: Upload Individual Runtime Benchmarks
        uses: actions/upload-artifact@v5
        if: always()
        with:
          name: benchmark-MatrixTests
          path: |
            docs/docs/benchmarks/MatrixTests.md
            docs/static/benchmarks/MatrixTests.json
          retention-days: 90

      - name: Upload Individual Runtime Benchmarks
        uses: actions/upload-artifact@v5
        if: always()
        with:
          name: benchmark-MassiveParallelTests
          path: |
            docs/docs/benchmarks/MassiveParallelTests.md
            docs/static/benchmarks/MassiveParallelTests.json
          retention-days: 90

      - name: Upload Individual Runtime Benchmarks
        uses: actions/upload-artifact@v5
        if: always()
        with:
          name: benchmark-SetupTeardownTests
          path: |
            docs/docs/benchmarks/SetupTeardownTests.md
            docs/static/benchmarks/SetupTeardownTests.json
          retention-days: 90

      - name: Upload Build Benchmark
        uses: actions/upload-artifact@v5
        if: always()
        with:
          name: benchmark-BuildTime
          path: |
            docs/docs/benchmarks/BuildTime.md
            docs/static/benchmarks/BuildTime.json
          retention-days: 90

      - name: Upload Summary Files
        uses: actions/upload-artifact@v5
        if: always()
        with:
          name: benchmark-summary
          path: |
            docs/docs/benchmarks/index.md
            docs/static/benchmarks/latest.json
            docs/static/benchmarks/summary.json
            docs/static/benchmarks/historical.json
          retention-days: 90

      - name: Generate Benchmark List
        id: benchmark_list
        run: |
          # Read the summary.json file and format it for the PR body
          SUMMARY=$(cat docs/static/benchmarks/summary.json)
          RUNTIME_BENCHMARKS=$(echo "$SUMMARY" | grep -A 100 '"runtime"' | grep -o '"[^"]*Tests"' | sed 's/"//g' | sed 's/^/- /' | tr '\n' '|')
          BUILD_BENCHMARKS=$(echo "$SUMMARY" | grep -A 100 '"build"' | grep -o '"[^"]*"' | sed 's/"//g' | grep -v 'runtime\|build\|timestamp\|environment' | sed 's/^/- /' | tr '\n' '|')

          # Replace | with newlines for proper formatting
          RUNTIME_LIST=$(echo "$RUNTIME_BENCHMARKS" | sed 's/|/\n/g')
          BUILD_LIST=$(echo "$BUILD_BENCHMARKS" | sed 's/|/\n/g')

          # Set output with proper escaping
          echo "runtime_benchmarks<<EOF" >> $GITHUB_OUTPUT
          echo "$RUNTIME_LIST" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

          echo "build_benchmarks<<EOF" >> $GITHUB_OUTPUT
          echo "$BUILD_LIST" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Check for Changes
        id: check_changes
        run: |
          git add docs/docs/benchmarks/ docs/static/benchmarks/
          if git diff --quiet --staged; then
            echo "has_changes=false" >> $GITHUB_OUTPUT
            echo "No benchmark changes detected"
          else
            echo "has_changes=true" >> $GITHUB_OUTPUT
            echo "Benchmark changes detected"
          fi

      - name: Create Pull Request
        if: steps.check_changes.outputs.has_changes == 'true'
        id: create_pr
        uses: peter-evans/create-pull-request@v8
        with:
          token: ${{ secrets.ADMIN_TOKEN }}
          commit-message: 'chore: update benchmark results'
          branch: automated-benchmarks-update
          delete-branch: true
          title: 'ðŸ¤– Update Benchmark Results'
          body: |
            ## Automated Benchmark Update

            This PR updates the benchmark documentation with the latest results from the Speed Comparison workflow.

            ### Benchmarks Produced

            Individual benchmark artifacts are available for download:
            - `benchmark-DataDrivenTests`
            - `benchmark-AsyncTests`
            - `benchmark-ScaleTests`
            - `benchmark-MatrixTests`
            - `benchmark-MassiveParallelTests`
            - `benchmark-SetupTeardownTests`
            - `benchmark-BuildTime`
            - `benchmark-summary` (aggregated overview)

            #### Runtime Benchmarks
            ${{ steps.benchmark_list.outputs.runtime_benchmarks }}

            #### Build Benchmarks
            ${{ steps.benchmark_list.outputs.build_benchmarks }}

            ### Changes
            - Updated benchmark data in `docs/static/benchmarks/latest.json`
            - Updated historical trends in `docs/static/benchmarks/historical.json`
            - Regenerated benchmark documentation in `docs/docs/benchmarks/index.md`
            - Updated benchmark summary in `docs/static/benchmarks/summary.json`

            ### Workflow Run
            - **Run ID**: ${{ github.run_id }}
            - **Triggered**: ${{ github.event_name }}
            - **Date**: ${{ github.event.head_commit.timestamp }}

            ---

            ðŸ¤– This PR was automatically created and will be merged automatically once CI checks pass.
          labels: |
            automated
            benchmarks
            documentation
            ignore-for-release
          draft: false

      - name: Merge PR Immediately
        if: steps.check_changes.outputs.has_changes == 'true' && steps.create_pr.outputs.pull-request-number != ''
        env:
          GH_TOKEN: ${{ secrets.ADMIN_TOKEN }}
        run: |
          # Wait a moment for PR to be fully created
          sleep 5
          gh pr merge ${{ steps.create_pr.outputs.pull-request-number }} --squash --delete-branch --admin

